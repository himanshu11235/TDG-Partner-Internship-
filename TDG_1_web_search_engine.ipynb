{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TDG_1_web_search_engine.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPVZCaRVVhUoud5rgltIqJR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/himanshu11235/TDG-Partner-Internship-/blob/master/TDG_1_web_search_engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIQWLvTHCJW-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import requests\n",
        "import re\n",
        "import string\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCIsCrpV6Cw2",
        "colab_type": "code",
        "outputId": "ba15df5b-7c9d-4919-ee40-104564a4450c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8MMi41gEFTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#***** functions\n",
        "# FUNCTIONAL FORM OF INDEX\n",
        "def index(urls):\n",
        "  urls = set(urls)\n",
        "  BOWs = {}\n",
        "  for url in urls:\n",
        "    x = requests.get(url)\n",
        "    pge_txt = x.text\n",
        "    # untabify\n",
        "    clean = re.compile('<.*?>')\n",
        "    pge_txt = re.sub(clean, '', pge_txt)\n",
        "    # normalize\n",
        "    #lower\n",
        "    pge_txt = pge_txt.lower()\n",
        "    # removing numbers\n",
        "    pge_txt = re.sub(r'\\d', '', pge_txt)\n",
        "    #removing special characters !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
        "    to_remove = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\n'\n",
        "    table = {ord(char): ' ' for char in to_remove}\n",
        "    pge_txt = pge_txt.translate(table)\n",
        "    #strip\n",
        "    pge_txt = pge_txt.strip()\n",
        "    #tokenize\n",
        "    pge_wrd_lst = nltk.tokenize.word_tokenize(pge_txt)\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    pge_wrd_lst = [i for i in pge_wrd_lst if i not in stop_words]\n",
        "    BOW_dict = {}\n",
        "    for i in pge_wrd_lst :\n",
        "      if i in BOW_dict:\n",
        "        pass\n",
        "      else:\n",
        "        BOW_dict[i] = pge_wrd_lst.count(i) \n",
        "    \n",
        "    #lemmatization\n",
        "    lemmetizer=WordNetLemmatizer()\n",
        "    pge_lemmatized = [lemmetizer.lemmatize(i) for i in pge_wrd_lst]\n",
        "    BOW_dict_lemmatized = {}\n",
        "    for i in pge_lemmatized :\n",
        "      if i in BOW_dict_lemmatized:\n",
        "        pass\n",
        "      else:\n",
        "        BOW_dict_lemmatized[i] = pge_lemmatized.count(i) \n",
        "    \n",
        "    BOWs[url] = [BOW_dict, BOW_dict_lemmatized]\n",
        "  return BOWs\n",
        "\n",
        "\n",
        "# FUNCTIONAL FORM OF QUERRY\n",
        "def search(querry, BOWs):\n",
        "  lemmatised_scores = {}\n",
        "  non_lemmatised_scores = {}\n",
        "  for link in BOWs:\n",
        "    ## page vector dict\n",
        "    bow = BOWs[link][0]\n",
        "    bowl = BOWs[link][1]\n",
        "    ## querry vector dict\n",
        "    querry_dict = {}\n",
        "    for i in querry :\n",
        "      if i in querry_dict:\n",
        "        pass\n",
        "      else:\n",
        "        querry_dict[i] = querry.count(i)  \n",
        "    #non_lemmatized\n",
        "    union = set(bow).union(set(querry_dict))\n",
        "    L = len(union)\n",
        "    dotp = 0\n",
        "    for i in querry_dict:\n",
        "      if i in bow :\n",
        "        dotp += querry_dict[i]*bow[i]\n",
        "      else:\n",
        "        pass\n",
        "    l1 = 0\n",
        "    for i in bow:\n",
        "      l1 = l1 + bow[i]*bow[i]\n",
        "    l1 = math.sqrt(l1)\n",
        "    l2 = 0\n",
        "    for i in querry_dict:\n",
        "      l2 = l2 + querry_dict[i]*querry_dict[i]\n",
        "    l2 = math.sqrt(l2)\n",
        "    cosine = float(dotp)/(l1*l2)\n",
        "    non_lemmatised_scores[link] = cosine\n",
        "    #lemmatized\n",
        "    union = set(bowl).union(set(querry_dict))\n",
        "    L = len(union)\n",
        "    dotp = 0\n",
        "    for i in querry_dict:\n",
        "      if i in bowl :\n",
        "        dotp += querry_dict[i]*bowl[i]\n",
        "      else:\n",
        "        pass\n",
        "    l1 = 0\n",
        "    for i in bowl:\n",
        "      l1 = l1 + bowl[i]*bowl[i]\n",
        "    l1 = math.sqrt(l1)\n",
        "    l2 = 0\n",
        "    for i in querry_dict:\n",
        "      l2 = l2 + querry_dict[i]*querry_dict[i]\n",
        "    l2 = math.sqrt(l2)\n",
        "    cosine = float(dotp)/(l1*l2)\n",
        "    lemmatised_scores[link] = cosine\n",
        "  return [lemmatised_scores, non_lemmatised_scores]\n",
        "\n",
        "####### FUNCTION FOR SORTING \n",
        "def f(element):\n",
        "  return -element[1]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XY4icetbKg5U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CLASS CONTAINING INDEX AND QUERRY AND HENCE WHOLE SEARCH ENGINE\n",
        "class websearchengine:\n",
        "  def __init__(self):\n",
        "    self.BOWs = {}\n",
        "\n",
        "  def index(self, urls):\n",
        "    urls = set(urls)\n",
        "    self.BOWs = {}\n",
        "    for url in urls:\n",
        "      x = requests.get(url)\n",
        "      pge_txt = x.text\n",
        "      # untabify\n",
        "      clean = re.compile('<.*?>')\n",
        "      pge_txt = re.sub(clean, '', pge_txt)\n",
        "      # normalize\n",
        "      #lower\n",
        "      pge_txt = pge_txt.lower()\n",
        "      # removing numbers\n",
        "      pge_txt = re.sub(r'\\d', '', pge_txt)\n",
        "      #removing special characters !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
        "      to_remove = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\n'\n",
        "      table = {ord(char): ' ' for char in to_remove}\n",
        "      pge_txt = pge_txt.translate(table)\n",
        "      #strip\n",
        "      pge_txt = pge_txt.strip()\n",
        "      #tokenize\n",
        "      pge_wrd_lst = nltk.tokenize.word_tokenize(pge_txt)\n",
        "      stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "      pge_wrd_lst = [i for i in pge_wrd_lst if i not in stop_words]\n",
        "      BOW_dict = {}\n",
        "      for i in pge_wrd_lst :\n",
        "        if i in BOW_dict:\n",
        "          pass\n",
        "        else:\n",
        "          BOW_dict[i] = pge_wrd_lst.count(i) \n",
        "      \n",
        "      #lemmatization\n",
        "      lemmetizer=WordNetLemmatizer()\n",
        "      pge_lemmatized = [lemmetizer.lemmatize(i) for i in pge_wrd_lst]\n",
        "      BOW_dict_lemmatized = {}\n",
        "      for i in pge_lemmatized :\n",
        "        if i in BOW_dict_lemmatized:\n",
        "          pass\n",
        "        else:\n",
        "          BOW_dict_lemmatized[i] = pge_lemmatized.count(i) \n",
        "      \n",
        "      self.BOWs[url] = [BOW_dict, BOW_dict_lemmatized]\n",
        "    \n",
        "  def search(self,querry):\n",
        "    lemmatised_scores  =  {}\n",
        "    non_lemmatised_scores = {}\n",
        "    if(len(self.BOWs) == 0):\n",
        "      print(\"No URLs to traverse\")\n",
        "      return\n",
        "    for link in self.BOWs:\n",
        "      ## page vector dict\n",
        "      bow = self.BOWs[link][0]\n",
        "      bowl = self.BOWs[link][1]\n",
        "      ## querry vector dict\n",
        "      querry_dict = {}\n",
        "      for i in querry :\n",
        "        if i in querry_dict:\n",
        "          pass\n",
        "        else:\n",
        "          querry_dict[i] = querry.count(i)  \n",
        "      #non_lemmatized\n",
        "      union = set(bow).union(set(querry_dict))\n",
        "      L = len(union)\n",
        "      dotp = 0\n",
        "      for i in querry_dict:\n",
        "        if i in bow :\n",
        "          dotp += querry_dict[i]*bow[i]\n",
        "        else:\n",
        "          pass\n",
        "      l1 = 0\n",
        "      for i in bow:\n",
        "        l1 = l1 + bow[i]*bow[i]\n",
        "      l1 = math.sqrt(l1)\n",
        "      l2 = 0\n",
        "      for i in querry_dict:\n",
        "        l2 = l2 + querry_dict[i]*querry_dict[i]\n",
        "      l2 = math.sqrt(l2)\n",
        "      \n",
        "      cosine = float(dotp)/(l1*l2)\n",
        "      non_lemmatised_scores[link] = cosine\n",
        "      #lemmatized\n",
        "      union = set(bowl).union(set(querry_dict))\n",
        "      L = len(union)\n",
        "      dotp = 0\n",
        "      for i in querry_dict:\n",
        "        if i in bowl :\n",
        "          dotp += querry_dict[i]*bowl[i]\n",
        "        else:\n",
        "          pass\n",
        "      l1 = 0\n",
        "      for i in bowl:\n",
        "        l1 = l1 + bowl[i]*bowl[i]\n",
        "      l1 = math.sqrt(l1)\n",
        "      l2 = 0\n",
        "      for i in querry_dict:\n",
        "        l2 = l2 + querry_dict[i]*querry_dict[i]\n",
        "      l2 = math.sqrt(l2)\n",
        "      cosine = float(dotp)/(l1*l2)\n",
        "      lemmatised_scores[link] = cosine\n",
        "    lemmatised_scores = sorted(lemmatised_scores.items(), key = f)\n",
        "    non_lemmatised_scores = sorted(non_lemmatised_scores.items(), key = f)\n",
        "    return {'lemmatised_scores_dictionary' : lemmatised_scores, 'non_lemmatised_score_dictionary':non_lemmatised_scores}\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7qVSs1jAkc2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "2448b09e-0656-4589-8764-d3f98ee2fb36"
      },
      "source": [
        "urls = ['https://www.programiz.com/python-programming/methods/list/count', 'https://www.geeksforgeeks.org/python-sort-python-dictionaries-by-key-or-value/','https://www.programiz.com/python-programming/methods/set/union', 'https://docs.python.org/3/library/math.html']\n",
        "querries = ['and', 'web'] \n",
        "engine = websearchengine()\n",
        "engine.index(urls)\n",
        "sorted_scores_dict = engine.search(querries)\n",
        "if sorted_scores_dict != None:\n",
        "  ## OUTPUT USING LEMMATISATION\n",
        "  lemmatised_scores_dictionary = sorted_scores_dict['lemmatised_scores_dictionary']\n",
        "  print(\"OUTPUT USING LEMMATISATION\")\n",
        "  for i in lemmatised_scores_dictionary:\n",
        "    print('LINK : ', i[0], 'COSINE SCORE : ', i[1])\n",
        "  # OUTPUT WITHOUT USING LEMMATISATION\n",
        "  non_lemmatised_score_dictionary = sorted_scores_dict['non_lemmatised_score_dictionary']\n",
        "  print(\"OUTPUT WITHOUT USING LEMMATISATION\")\n",
        "  for i in non_lemmatised_score_dictionary:\n",
        "    print('LINK : ', i[0], 'COSINE SCORE : ', i[1])\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OUTPUT USING LEMMATISATION\n",
            "LINK :  https://www.geeksforgeeks.org/python-sort-python-dictionaries-by-key-or-value/ COSINE SCORE :  0.007362001982874519\n",
            "LINK :  https://www.programiz.com/python-programming/methods/list/count COSINE SCORE :  0.0010394572356857514\n",
            "LINK :  https://www.programiz.com/python-programming/methods/set/union COSINE SCORE :  0.0010279148200143544\n",
            "LINK :  https://docs.python.org/3/library/math.html COSINE SCORE :  0.0\n",
            "OUTPUT WITHOUT USING LEMMATISATION\n",
            "LINK :  https://www.geeksforgeeks.org/python-sort-python-dictionaries-by-key-or-value/ COSINE SCORE :  0.007546149041401722\n",
            "LINK :  https://www.programiz.com/python-programming/methods/list/count COSINE SCORE :  0.0010478830333401758\n",
            "LINK :  https://www.programiz.com/python-programming/methods/set/union COSINE SCORE :  0.001036441400823938\n",
            "LINK :  https://docs.python.org/3/library/math.html COSINE SCORE :  0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Os7bKK5yL5c1",
        "colab_type": "text"
      },
      "source": [
        "PROBLEMS OBSERVED:\n",
        "1. We are using term frequency for creating bag of  words. But this approach does not assume anything about the relative importance of the words in a webpage. \n",
        "for that we can use term frequency–inverse document frequency to create out bag of words. this might give more precise results.\n",
        "2. we are searching solely on the basis of correlation between bag of words of a page and the querry. This does not capture semantics and which sometimes becomes important. \n",
        "for this we can use word2vec word embeddings to and calculate the similarity based on these vector representations of words rather than depending solely on the frequency of words.\n",
        "3. The normalisation of the page is leading to some wierd words and hence we need to refine our methods.\n",
        "For now I am thinking of creating a more rigorous library to strictly extract the text from a html page which will be built by taking reference of whole HTML and CSS and how different elements are actually created in a webpage.\n",
        "   \n",
        "\n",
        "\n",
        "\n",
        "References\n",
        "https://docs.python.org/3/library/re.html\n",
        "https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwObi02uL3WB",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}